{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week2_4 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 커스텀 모듈(`helper.py`)에서 **클래스와 함수를 임포트**할 수 있다.\n",
        "- **autograd**의 개념 복습\n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- train() 함수에 **epoch, scheduler, grad_clipping**을 추가할 수 있다.\n",
        "- **validate() 함수를 구현**할 수 있다.\n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- train() 함수를 사용해 데이터를 **4 epoch 학습**할 수 있다. \n",
        "- **predict 함수를 구현**할 수 있다. \n",
        "- **evaluation metric 구현**할 수 있다. \n",
        "    - accuracy\n",
        "\n",
        "\n",
        "\n",
        "### Reference\n",
        "- [Pytorch Autograd Explain official document](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:47.370876Z",
          "start_time": "2022-02-02T04:01:46.520392Z"
        },
        "id": "KSX-wQA1RD1h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:47.375658Z",
          "start_time": "2022-02-02T04:01:47.372242Z"
        },
        "id": "MH7RJjtZXOHf"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 7777\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-31T13:07:00.849353Z",
          "start_time": "2022-01-31T13:06:56.187962Z"
        },
        "id": "62plMahMWr0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40d18d9-62d5-4a0f-d07d-461c28169442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 23.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WagJcj-Ud4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c135258-c83b-4edc-d196-0b61f8d21203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnETqIqdVApF"
      },
      "outputs": [],
      "source": [
        "# 어제 자신이 구현한 help.py 모듈 경로를 입력\n",
        "sys.path.append(\"/content/drive/MyDrive/Mymodules\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sys.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNGxtPX0eB7K",
        "outputId": "4346085d-c0c9-47b3-c10f-3280a06f5f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Mymodules']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('help.py','wb').write(src)"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "5xqzSwpllC19",
        "outputId": "d8f974cf-5fd1-4ff5-8cee-999997f54ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aae8be10-c702-4c51-abae-f9413609d344\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aae8be10-c702-4c51-abae-f9413609d344\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving help.py to help.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3622"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:49.735578Z",
          "start_time": "2022-02-02T04:01:49.475969Z"
        },
        "id": "N84mZeYMUFxJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "d321ab52be094027af6db242ff73413e",
            "56c853b0280e441c90ffb43fa4954011",
            "0e63dd2edc114376a60458b8797771aa",
            "b3de612915a24db89422dcae7f0f0b9b",
            "95aa2a43b7454684b75e83185467ff47",
            "51099c4718414830bd21e515e6071658",
            "1d414b3aa3ae442e8ec3a3f87e6eac38",
            "0acbbe906da54f21abadc65c8b834256",
            "a2111629362c4f509db209af951162ac",
            "12d295e6f0f942c4885d178080cd5026",
            "8311bf8af344468e8ecd29f301f09dac",
            "a4c086c01008416ca577d8a5e25102ed",
            "f3dd49f1ad704bdfa3b9ff4bd4ce8226",
            "fd22a92c51df4466a302a4e5bcf15fd0",
            "b4556b72d07a4ddeb49986890571908b",
            "583194522ba94cdc8ff5c6faca34c3bb",
            "6dc3620bdaeb4847b87d72d165180145",
            "3d4a53b7fba240778b6f2dfb071ac3d7",
            "74f3645f04904775ae3599228c9210dc",
            "672c3b15df37497c9b4ed0f61faa204b",
            "12ab5a5d11a44714bca975d5f4dbf985",
            "88fbab4df4034b0d82b1ad4e978b7bcf",
            "20d3435db01a4b0fac9b477aeb87662d",
            "8d2de39a2f8b45bb8fca40c35df93e45",
            "9e05a6d35e274afdbc189b02424a1b3b",
            "b7126dea45df4230a2098b97de1bdd4c",
            "a485175227d24c59b60407a55408d3cd",
            "9511c53f58944349879b8767e24b6df9",
            "ef76cb2fe0404ec5b11b698ea2fd0549",
            "a6018b7bd6d34a2d9c27b1e158f453f9",
            "0e0696922bb347a18cca1eafb8478cf0",
            "201c39ee3f3b4f15a244c6c20e7d31cf",
            "76f4595fcee24b81ac6f6a721b17b4d4",
            "8635b140a5bb409eb2b4d3ca140998e1",
            "6f5a4fae5dee49a5af69ce6ff665f4a1",
            "8095139339064856a0a90a0b593a154c",
            "1978e4da33994b36901469b5557a42e7",
            "d1623787f4f74648bd84828f433ef52d",
            "3828164fe1974f2eb130fe631a3e93a6",
            "e6e79b8996104020882a683107d7ba03",
            "2318b75e89f949bca31fd89fbee43be0",
            "0aed0d693f2b48a8b96d82e1d072c129",
            "8d3d1c356ed14751b6ad125448b4e100",
            "13e1d0c68d904ad4bf26ad1c1e0b55d1"
          ]
        },
        "outputId": "91c64f74-95a8-4448-d1aa-a4219f69805b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d321ab52be094027af6db242ff73413e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4c086c01008416ca577d8a5e25102ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20d3435db01a4b0fac9b477aeb87662d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8635b140a5bb409eb2b4d3ca140998e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# help 모듈을 import하면 이전에 구현했던 다양한 함수 및 클래스를 사용할 수 있음 \n",
        "# 함수: set_device()\n",
        "# 함수: custom_collate_fn() \n",
        "# 클래스: CustomDataset\n",
        "# 클래스: CustomClassifier\n",
        "# 가 import 됨\n",
        "\n",
        "\n",
        "from torch.utils.data import RandomSampler, SequentialSampler, Dataset\n",
        "from help import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:49.771743Z",
          "start_time": "2022-02-02T04:01:49.736866Z"
        },
        "id": "oR5EWmh5UFxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6424a65-2159-4969-dab7-5736a581dc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla K80\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# device\n",
        "device = set_device(torch)\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkNxrCV45Q3m"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YBUQykS5Q3n"
      },
      "source": [
        "### 모듈에서 클래스와 함수를 임포트해 다음을 구현\n",
        "- train_dataset, train_dataloader\n",
        "- valid_dataset, valid_dataloader\n",
        "- test_dataset, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataframe 다운로드\n",
        "!wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/e56006adfac42f8a2975db0ebbe60eacbe1c6b11/data/sample_df.csv"
      ],
      "metadata": {
        "id": "cjNksUEwGACb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb7242c-80be-41d0-8fda-e3a76d1bd1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-03 16:32:15--  https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/e56006adfac42f8a2975db0ebbe60eacbe1c6b11/data/sample_df.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 971625 (949K) [text/plain]\n",
            "Saving to: ‘sample_df.csv’\n",
            "\n",
            "\rsample_df.csv         0%[                    ]       0  --.-KB/s               \rsample_df.csv       100%[===================>] 948.85K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-03-03 16:32:16 (20.6 MB/s) - ‘sample_df.csv’ saved [971625/971625]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataframe 다운로드\n",
        "!wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/main/data/sample_df_test.csv"
      ],
      "metadata": {
        "id": "kXfk8ZEHGB0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebf2e8c-7c05-4db4-f28f-861ff3e7c4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-03 16:32:16--  https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/main/data/sample_df_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101383 (99K) [text/plain]\n",
            "Saving to: ‘sample_df_test.csv’\n",
            "\n",
            "\rsample_df_test.csv    0%[                    ]       0  --.-KB/s               \rsample_df_test.csv  100%[===================>]  99.01K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-03-03 16:32:17 (7.07 MB/s) - ‘sample_df_test.csv’ saved [101383/101383]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:53.037044Z",
          "start_time": "2022-02-02T04:01:52.707669Z"
        },
        "id": "KVo5dPnmUFxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e526d8-ddc3-4415-81dc-af8aaec3cd77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape : (10000, 3)\n",
            "test shape : (1000, 3)\n"
          ]
        }
      ],
      "source": [
        "# 학습 & 평가 데이터셋 로드\n",
        "# 학습 및 평가 샘플 데이터 개수는 각각 10,000개, 1,000개\n",
        "\n",
        "df_train = pd.read_csv('sample_df.csv')\n",
        "df_test = pd.read_csv('sample_df_test.csv')\n",
        "\n",
        "print(f\"train shape : {df_train.shape}\")\n",
        "print(f\"test shape : {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5lLQqd-Im1e7",
        "outputId": "ca878f1e-1551-4b46-c677-df2dc253b737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fdc4c9f1-a3a8-4e37-b1b0-6fb284fe4c44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8525343</td>\n",
              "      <td>나 이거 더빙을 누가하는지 모르고 봤는데 왠지 더빙이 구리더라...더빙이 너무 별로였음.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4572888</td>\n",
              "      <td>현암이 소지섭이었으면 좋았겠는데..스토리각색도 좀 깔끔하게...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8504845</td>\n",
              "      <td>ㅎㅎㅎ 대단하네 ㅜ,.ㅡ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5003367</td>\n",
              "      <td>이거보고 돈날린 기억이...........</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3015049</td>\n",
              "      <td>한국영화 어쩌다 이지경까지 ㅠㅠ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdc4c9f1-a3a8-4e37-b1b0-6fb284fe4c44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdc4c9f1-a3a8-4e37-b1b0-6fb284fe4c44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdc4c9f1-a3a8-4e37-b1b0-6fb284fe4c44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  8525343  나 이거 더빙을 누가하는지 모르고 봤는데 왠지 더빙이 구리더라...더빙이 너무 별로였음.      0\n",
              "1  4572888                현암이 소지섭이었으면 좋았겠는데..스토리각색도 좀 깔끔하게...      0\n",
              "2  8504845                                      ㅎㅎㅎ 대단하네 ㅜ,.ㅡ      0\n",
              "3  5003367                            이거보고 돈날린 기억이...........      0\n",
              "4  3015049                                  한국영화 어쩌다 이지경까지 ㅠㅠ      0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:53.085720Z",
          "start_time": "2022-02-02T04:01:53.081413Z"
        },
        "id": "Ql82Ew2VUFxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecc1db9-aa7f-4a75-e45f-680bc2738174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset len: 10000\n",
            "Train Dataset 1st element: ('나 이거 더빙을 누가하는지 모르고 봤는데 왠지 더빙이 구리더라...더빙이 너무 별로였음.', 0)\n",
            "Test Dataset len: 1000\n",
            "Test Dataset 1st element: ('신용문객잔 보고 후속편인줄 알고 봤더만 완전 개판이네 18.. 이련결 그냥 절에나 쳐 들어 가라.. 회오리에서 싸우는 신 참 가관이더라 .. 서극도 완전 쓰레기 감독이 다 됐구나.. 액션도 쓰레기고 배우들 연기도 참 가관이더라 18', 0)\n"
          ]
        }
      ],
      "source": [
        "# Dataset 구현\n",
        "# helper.py에 있는 CustomDataset 활용하여 train datset, test dataset 만들기\n",
        "\n",
        "train_dataset = CustomDataset(list(df_train['document']), list(df_train['label']))\n",
        "test_dataset = CustomDataset(list(df_test['document']), list(df_test['label']))\n",
        "\n",
        "print(f\"Train Dataset len: {len(train_dataset)}\")\n",
        "print(f\"Train Dataset 1st element: {train_dataset[0]}\")\n",
        "\n",
        "print(f\"Test Dataset len: {len(test_dataset)}\")\n",
        "print(f\"Test Dataset 1st element: {test_dataset[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:53.152070Z",
          "start_time": "2022-02-02T04:01:53.145410Z"
        },
        "id": "7WUY6h8WUFxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d1c8a5-dd42-40f7-d2b8-8e87985c8b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset len: 9000\n",
            "Valid dataset len: 1000\n"
          ]
        }
      ],
      "source": [
        "# Train Dataset을 학습과 검증 셋으로 분리\n",
        "# 학습 셋과 검증 셋의 비율은 9:1\n",
        "# torch.utils.data에서 제공되는 데이터 세트를 임의로 분할할 수 있는 함수 찾아서 사용\n",
        "# (train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [n_train, n_valid]))\n",
        "\n",
        "n_train_sample = df_train.shape[0]\n",
        "\n",
        "n_train = int(n_train_sample*0.9)\n",
        "n_valid = n_train_sample - n_train \n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [n_train, n_valid])\n",
        "\n",
        "print(f\"Train dataset len: {len(train_dataset)}\")\n",
        "print(f\"Valid dataset len: {len(valid_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:53.268838Z",
          "start_time": "2022-02-02T04:01:53.263780Z"
        },
        "id": "H5nc7SpTUFxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0a0e68-4a03-47a4-eae4-0685dc210527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataloader # steps: 282\n",
            "Valid dataloader # steps: 16\n",
            "Test dataloader # steps: 16\n"
          ]
        }
      ],
      "source": [
        "# DataLoader 구현\n",
        "# train과 validation의 batch size는 각각 32, 64로 설정\n",
        "# test의 batch size는 validation과 동일\n",
        "# train에 사용할 DataLoader에서는 sampler로 RandomSampler 사용\n",
        "# validation과 test에 사용할 DataLoader에서는 sampler로 SequentialSampler 사용\n",
        "# 모든 DataLoader의 collate_fn은 helper.py에 있는 custom_collate_fn 사용\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 64\n",
        "test_batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size = train_batch_size,\n",
        "                              sampler = RandomSampler(train_dataset),\n",
        "                              collate_fn = custom_collate_fn\n",
        "                              )\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset,\n",
        "                              batch_size = valid_batch_size,\n",
        "                              sampler = SequentialSampler(valid_dataset),\n",
        "                              collate_fn = custom_collate_fn\n",
        "                              )\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             batch_size = test_batch_size,\n",
        "                             sampler = SequentialSampler(test_dataset),\n",
        "                             collate_fn = custom_collate_fn\n",
        "                             )\n",
        "\n",
        "print(f\"Train dataloader # steps: {len(train_dataloader)}\")\n",
        "print(f\"Valid dataloader # steps: {len(valid_dataloader)}\")\n",
        "print(f\"Test dataloader # steps: {len(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kEgqvBIUFxN"
      },
      "source": [
        "### `auto_grad` 개념 복습\n",
        "- torch의 `auto_grad` 기능\n",
        "    - pytorch는 `requires_grad` 파리미터의 값이 True인 텐서에 한해서 미분값을 자동으로 계산한다.\n",
        "    - 미분값은 `loss.backward()` 가 호출될 때 자동으로 계산된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-31T13:45:23.502936Z",
          "start_time": "2022-01-31T13:45:20.029987Z"
        },
        "id": "oYjYpQ1DUFxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "4f1f9e5305a0409bad443be219fa9d67",
            "75e709e7009c4a81b0b8875abb75793b",
            "2c1be09835754bbc9869b566d440d64e",
            "3807d440dde3429ca7d9ce88ca60c0ed",
            "80d432ca46fc40dbb1efdbafe5fc057e",
            "1a4448fe121d4aaca1f0864f3d54fe77",
            "20b0b597d83640c9bef7b865f8b2cbc4",
            "a9d70f90f5ca4631bdd6660180d10314",
            "4faf29530d1e4c08aed80615a12b7381",
            "f83110d2ed8245278b94d1426602fd6f",
            "4827d1f93fb14b55967034c79ce3c719"
          ]
        },
        "outputId": "f45be42f-a6f3-4014-9cee-c813c758492f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f1f9e5305a0409bad443be219fa9d67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# helper.py에 있는 CustomClassifier 모델을 로드해 model_freeze 변수에 instance를 생성\n",
        "# hidden_size=768\n",
        "# n_label=2\n",
        "# freeze_base=True (X)\n",
        "# model_freeze이지만, help.py의 CustomClassifier는 unfreeze model이므로, 실질적으로 unfreeze_model이다!\n",
        "\n",
        "model_freeze = CustomClassifier(hidden_size = 768, n_label=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-31T13:45:34.604914Z",
          "start_time": "2022-01-31T13:45:34.586711Z"
        },
        "id": "XxNFh8KZUFxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3311c156-e671-4894-ab82-794a7958bebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name:bert.embeddings.word_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32000, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.embeddings.position_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([512, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.embeddings.token_type_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.embeddings.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.embeddings.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.pooler.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:bert.pooler.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:classifier.0.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32, 768])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:classifier.0.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:classifier.3.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2, 32])\n",
            "param.requries_grad:True\n",
            "=====\n",
            "name:classifier.3.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2])\n",
            "param.requries_grad:True\n",
            "=====\n"
          ]
        }
      ],
      "source": [
        "# model_freeze 모델의 모든 파라미터를 출력해보고 아래 질문에 답해 보자 (named_parameters)\n",
        "\n",
        "for name, param in model_freeze.named_parameters():\n",
        "  print(f'name:{name}') \n",
        "  print(type(param)) \n",
        "  print(f'param.shape:{param.shape}') \n",
        "  print(f'param.requries_grad:{param.requires_grad}') \n",
        "  print('=====')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KloNNAKI5Q3r"
      },
      "source": [
        "### `auto_grad` 개념 및 모델 구조 복습을 위해 다음 항목에 답해 보자\n",
        "- `bert.encoder.layer.0.attention.self.query.weight` 텐서의 gradient는 True인 상태인가?\n",
        "> 맞다.\n",
        "- `classifier.0.weight` 텐서의 shape은? \n",
        "> torch.Size([32, 768])\n",
        "- `classifier.0.weight` 텐서는 freeze 상태인가 ? \n",
        "> 아니다.\n",
        "- `classifier.0.weight` 텐서의 gradient 값은 무엇인가? \n",
        "> None. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iIrHg1xUFxP"
      },
      "source": [
        "### 위 모델 (`model_freeze`)의 모든 파라미터의 gradient를 freeze 해보자"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_freeze.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "LULFe0FTT7Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-31T13:49:26.820569Z",
          "start_time": "2022-01-31T13:49:26.816511Z"
        },
        "id": "sHkaFgC8UFxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2ab0cf-2347-4053-e88a-4ab5c023f604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name:bert.embeddings.word_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32000, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.embeddings.position_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([512, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.embeddings.token_type_embeddings.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.embeddings.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.embeddings.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.0.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.1.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.2.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.3.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.4.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.5.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.6.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.7.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.8.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.9.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.10.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.query.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.query.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.key.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.key.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.value.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.self.value.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.intermediate.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.intermediate.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 3072])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.LayerNorm.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.encoder.layer.11.output.LayerNorm.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.pooler.dense.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:bert.pooler.dense.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:classifier.0.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32, 768])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:classifier.0.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([32])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:classifier.3.weight\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2, 32])\n",
            "param.requries_grad:False\n",
            "=====\n",
            "name:classifier.3.bias\n",
            "<class 'torch.nn.parameter.Parameter'>\n",
            "param.shape:torch.Size([2])\n",
            "param.requries_grad:False\n",
            "=====\n"
          ]
        }
      ],
      "source": [
        "# 모든 파라미터의 gradient를 freeze 해보고 제대로 변경되었는지 확인하기 위해 모델의 모든 파라미터를 출력해보자.\n",
        "\n",
        "for name, param in model_freeze.named_parameters():\n",
        "  print(f'name:{name}') \n",
        "  print(type(param)) \n",
        "  print(f'param.shape:{param.shape}') \n",
        "  print(f'param.requries_grad:{param.requires_grad}') \n",
        "  print('=====')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsMgM3sK5Q3t"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUn-6PFP5Q3t"
      },
      "source": [
        "### `scheduler` 를 생성 \n",
        "- 스케쥴러를 알기 전에 먼저 `epoch`의 개념을 알아야 한다. Epoch는 dataset를 **몇 번 반복**해 학습할 것인지를 의미한다. 만약 dataset의 개수가 2,000개이고 epoch을 2번 학습하게 되면 총 4,000개의 데이터를 학습하게 된다.   \n",
        "- 스케쥴러는 epoch에 따라 learning rate의 값을 조정하는 것을 의미한다. \n",
        "- 예를 들어 [여기](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)의 그림에서 볼 수 있듯이 `get_linear_schedule_with_warmup`는 특정 step까지는 learning rate를 천천히 상승시키다가 고점에 도달하면 다시 하락시킨다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FuADvuT5Q3t"
      },
      "source": [
        "### `model`, `optimizer`, `scheduler`를 초기화(=인스턴스 생성)하는 함수를 구현하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:59.217735Z",
          "start_time": "2022-02-02T04:01:59.210482Z"
        },
        "id": "-sE7xjYcRD1p"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import get_linear_schedule_with_warmup, get_constant_schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:01:59.549660Z",
          "start_time": "2022-02-02T04:01:59.545752Z"
        },
        "id": "2eTFXzy8VK9R"
      },
      "outputs": [],
      "source": [
        "# model:CustomClassifier 사용, hidden size는 768, label 개수는 2\n",
        "# optimizer: AdamW 사용, learning rate는 2e-5\n",
        "# scheduler: transformers.get_linear_schedule_with_warmup 함수 사용, 단, num_warmup_steps 매개 변수는 사용하지 않음 (num_warmup_steps = 0)\n",
        "\n",
        "def initializer(train_dataloader, epochs=2):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러를 초기화한 후 반환\n",
        "    \"\"\"\n",
        "    \n",
        "    model = CustomClassifier(hidden_size=768, n_label=2)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=2e-5\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = 0,\n",
        "                                                num_training_steps = total_steps)\n",
        "\n",
        "    return model, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz-8_5as5Q3u"
      },
      "source": [
        "### model, optimizer, scheduler의 파라미터 저장하는 함수를 구현하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:02:02.786877Z",
          "start_time": "2022-02-02T04:02:02.783726Z"
        },
        "id": "vIP1BjFA5Q3u"
      },
      "outputs": [],
      "source": [
        "# 모델 저장 함수 구현\n",
        "\n",
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n",
        "    file_name = f'{path}/model.ckpt.{epoch}'\n",
        "    \n",
        "    # torch.save 함수 참고\n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3BUrgtJ5Q3v"
      },
      "source": [
        "### `validate()` 함수 구현 \n",
        "- `validate()` 함수 내 model의 상태는 **evaluate**이어야 한다. evaluate 상태의 model은 dropout을 진행하지 않는다. \n",
        "- **forward**를 진행할 때 `with torch.no_grad(): ...` 설정해 미분 계산을 방지한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:02:11.636684Z",
          "start_time": "2022-02-02T04:02:11.631550Z"
        },
        "id": "VHpuV0CXUFxR"
      },
      "outputs": [],
      "source": [
        "# input: model, valid_dataloader\n",
        "# output: loss, 정확도\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def validate(model, valid_dataloader):\n",
        "\n",
        "  global loss_fct\n",
        "  \n",
        "  # 모델을 evaluate 모드로 설정 & device 할당\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  \n",
        "  total_loss, total_acc= 0,0\n",
        "      \n",
        "  for step, batch in enumerate(valid_dataloader):\n",
        "      \n",
        "      # tensor 연산 전, 각 tensor에 device 할당\n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "          \n",
        "      batch_input, batch_label = batch\n",
        "          \n",
        "      # gradient 계산하지 않고 forward 진행\n",
        "      with torch.no_grad():\n",
        "          logits = model.forward(batch_input['input_ids'], batch_input['attention_mask'], batch_input['token_type_ids'])\n",
        "          \n",
        "      # loss\n",
        "      loss = loss_fct(logits, batch_label)\n",
        "      total_loss += loss.item()\n",
        "      \n",
        "      # accuracy\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      preds = torch.argmax(probs, dim=1).flatten()\n",
        "      acc = (preds == batch_label).cpu().numpy().mean()\n",
        "      total_acc+=acc\n",
        "  \n",
        "  total_loss = total_loss/(step+1)\n",
        "  total_acc = total_acc/(step+1)*100\n",
        "\n",
        "  return total_loss, total_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NukaJc15UFxQ"
      },
      "source": [
        "### `train()` 함수에 `epoch`와 `clip_grad_norm` 추가\n",
        "- data_loader를 `epoch`만큼 반복하면서 학습하도록 `train()` 함수를 수정하라\n",
        "- `gradient cliping`은 미분 값 너무 큰 경우 gradient exploding되는 현상을 막기 위해 미분값이 `threshold`를 넘을 경우 특정 비율을 미분 값에 곱해 크기를 줄여준다.\n",
        "- Reference\n",
        "  - [clip_grad_norm_ official document](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "  - [그래디언트 클립핑 설명 한국어 블로그](https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-6/05-gradient-clipping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T04:02:10.624280Z",
          "start_time": "2022-02-02T04:02:10.615781Z"
        },
        "id": "ZvY5rxDKHQAp"
      },
      "outputs": [],
      "source": [
        "# 위에서 구현한 모델 저장 함수(save_checkpoint)와 validate 함수도 추가해보자\n",
        "\n",
        "loss_fct = CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_dataloader, valid_dataloader=None, epochs=2):\n",
        "        global scheduler, loss_fct\n",
        "        \n",
        "        # train_dataloaer 학습을 epochs만큼 반복\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            \n",
        "            # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n",
        "            total_loss, batch_loss, batch_count = 0,0,0\n",
        "        \n",
        "            # model을 train 모드로 설정 & device 할당\n",
        "            model.train()\n",
        "            model.to(device)\n",
        "            \n",
        "            # data iterator를 돌면서 하나씩 학습\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_count+=1\n",
        "                \n",
        "                # tensor 연산 전, 각 tensor에 device 할당\n",
        "                batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "                batch_input, batch_label = batch\n",
        "            \n",
        "                # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "                model.zero_grad()\n",
        "            \n",
        "                # forward\n",
        "                logits = model(**batch_input)\n",
        "            \n",
        "                # loss\n",
        "                loss = loss_fct(logits, batch_label)\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "                # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "                loss.backward()\n",
        "                \n",
        "                # gradient clipping 적용 (max_norm = 1)\n",
        "                clip_grad_norm_(model.parameters(),\n",
        "                                max_norm = 1)\n",
        "                \n",
        "                # optimizer & scheduler 업데이트\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                \n",
        "                # 배치 10개씩 처리할 때마다 평균 loss와 lr를 출력\n",
        "                if (step % 10 == 0 and step != 0):\n",
        "                    learning_rate = optimizer.param_groups[0]['lr']\n",
        "                    print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "\n",
        "                    # reset \n",
        "                    batch_loss, batch_count = 0,0\n",
        "\n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "            \n",
        "            if valid_dataloader is not None:\n",
        "                print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "                valid_loss, valid_acc = validate(model, valid_dataloader)\n",
        "                print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid Acc : {valid_acc:.2f}\")\n",
        "                print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "            \n",
        "            # checkpoint 저장\n",
        "            save_checkpoint('/content/drive/MyDrive/Mymodules', model, optimizer, scheduler, epoch, loss)\n",
        "                \n",
        "        print(\"Train Completed. End Program.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NWKzxIaf1QJ"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFWnii7a5Q3w"
      },
      "source": [
        "### 학습 데이터를 epoch 4까지 학습\n",
        "- 매 epoch마다 다음을 수행한다.\n",
        "  - 학습이 끝난 후 validate() 함수 실행 \n",
        "  - validate() 함수가 끝난 후 model save 함수 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:02:11.377612Z",
          "start_time": "2022-02-02T04:02:20.931961Z"
        },
        "id": "7Er1qKtsf1QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9463ce69-02c9-4e06-8d94-d4262f991548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 4 epochs: 1128\n",
            "*****Epoch 0 Train Start*****\n",
            "Epoch: 0, Step : 10, LR : 1.9804964539007094e-05, Avg Loss : 0.6525\n",
            "Epoch: 0, Step : 20, LR : 1.962765957446809e-05, Avg Loss : 0.5442\n",
            "Epoch: 0, Step : 30, LR : 1.945035460992908e-05, Avg Loss : 0.4889\n",
            "Epoch: 0, Step : 40, LR : 1.927304964539007e-05, Avg Loss : 0.4287\n",
            "Epoch: 0, Step : 50, LR : 1.9095744680851064e-05, Avg Loss : 0.3778\n",
            "Epoch: 0, Step : 60, LR : 1.891843971631206e-05, Avg Loss : 0.4031\n",
            "Epoch: 0, Step : 70, LR : 1.8741134751773053e-05, Avg Loss : 0.3817\n",
            "Epoch: 0, Step : 80, LR : 1.8563829787234043e-05, Avg Loss : 0.3845\n",
            "Epoch: 0, Step : 90, LR : 1.8386524822695038e-05, Avg Loss : 0.3774\n",
            "Epoch: 0, Step : 100, LR : 1.8209219858156032e-05, Avg Loss : 0.3760\n",
            "Epoch: 0, Step : 110, LR : 1.8031914893617023e-05, Avg Loss : 0.3789\n",
            "Epoch: 0, Step : 120, LR : 1.7854609929078013e-05, Avg Loss : 0.3902\n",
            "Epoch: 0, Step : 130, LR : 1.7677304964539008e-05, Avg Loss : 0.3992\n",
            "Epoch: 0, Step : 140, LR : 1.7500000000000002e-05, Avg Loss : 0.3640\n",
            "Epoch: 0, Step : 150, LR : 1.7322695035460996e-05, Avg Loss : 0.3803\n",
            "Epoch: 0, Step : 160, LR : 1.7145390070921987e-05, Avg Loss : 0.3974\n",
            "Epoch: 0, Step : 170, LR : 1.696808510638298e-05, Avg Loss : 0.3532\n",
            "Epoch: 0, Step : 180, LR : 1.6790780141843972e-05, Avg Loss : 0.3785\n",
            "Epoch: 0, Step : 190, LR : 1.6613475177304966e-05, Avg Loss : 0.3342\n",
            "Epoch: 0, Step : 200, LR : 1.6436170212765957e-05, Avg Loss : 0.3970\n",
            "Epoch: 0, Step : 210, LR : 1.625886524822695e-05, Avg Loss : 0.3288\n",
            "Epoch: 0, Step : 220, LR : 1.6081560283687945e-05, Avg Loss : 0.3742\n",
            "Epoch: 0, Step : 230, LR : 1.590425531914894e-05, Avg Loss : 0.3468\n",
            "Epoch: 0, Step : 240, LR : 1.572695035460993e-05, Avg Loss : 0.3206\n",
            "Epoch: 0, Step : 250, LR : 1.5549645390070924e-05, Avg Loss : 0.3320\n",
            "Epoch: 0, Step : 260, LR : 1.5372340425531915e-05, Avg Loss : 0.3534\n",
            "Epoch: 0, Step : 270, LR : 1.5195035460992908e-05, Avg Loss : 0.3874\n",
            "Epoch: 0, Step : 280, LR : 1.5017730496453902e-05, Avg Loss : 0.3811\n",
            "Epoch 0 Total Mean Loss : 0.3939\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 0.3664 Valid Acc : 84.34\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "Saving epoch 0 checkpoint at /content/drive/MyDrive/Mymodules/model.ckpt.0\n",
            "*****Epoch 1 Train Start*****\n",
            "Epoch: 1, Step : 10, LR : 1.4804964539007095e-05, Avg Loss : 0.2165\n",
            "Epoch: 1, Step : 20, LR : 1.4627659574468087e-05, Avg Loss : 0.2662\n",
            "Epoch: 1, Step : 30, LR : 1.4450354609929078e-05, Avg Loss : 0.2447\n",
            "Epoch: 1, Step : 40, LR : 1.427304964539007e-05, Avg Loss : 0.2537\n",
            "Epoch: 1, Step : 50, LR : 1.4095744680851065e-05, Avg Loss : 0.2422\n",
            "Epoch: 1, Step : 60, LR : 1.3918439716312057e-05, Avg Loss : 0.2665\n",
            "Epoch: 1, Step : 70, LR : 1.3741134751773051e-05, Avg Loss : 0.2698\n",
            "Epoch: 1, Step : 80, LR : 1.3563829787234044e-05, Avg Loss : 0.2521\n",
            "Epoch: 1, Step : 90, LR : 1.3386524822695038e-05, Avg Loss : 0.3207\n",
            "Epoch: 1, Step : 100, LR : 1.320921985815603e-05, Avg Loss : 0.2546\n",
            "Epoch: 1, Step : 110, LR : 1.3031914893617021e-05, Avg Loss : 0.2566\n",
            "Epoch: 1, Step : 120, LR : 1.2854609929078014e-05, Avg Loss : 0.2781\n",
            "Epoch: 1, Step : 130, LR : 1.2677304964539008e-05, Avg Loss : 0.2977\n",
            "Epoch: 1, Step : 140, LR : 1.25e-05, Avg Loss : 0.2134\n",
            "Epoch: 1, Step : 150, LR : 1.2322695035460995e-05, Avg Loss : 0.2066\n",
            "Epoch: 1, Step : 160, LR : 1.2145390070921987e-05, Avg Loss : 0.1940\n",
            "Epoch: 1, Step : 170, LR : 1.196808510638298e-05, Avg Loss : 0.2662\n",
            "Epoch: 1, Step : 180, LR : 1.1790780141843972e-05, Avg Loss : 0.3292\n",
            "Epoch: 1, Step : 190, LR : 1.1613475177304965e-05, Avg Loss : 0.2400\n",
            "Epoch: 1, Step : 200, LR : 1.1436170212765957e-05, Avg Loss : 0.2185\n",
            "Epoch: 1, Step : 210, LR : 1.1258865248226952e-05, Avg Loss : 0.2972\n",
            "Epoch: 1, Step : 220, LR : 1.1081560283687944e-05, Avg Loss : 0.2296\n",
            "Epoch: 1, Step : 230, LR : 1.0904255319148938e-05, Avg Loss : 0.2879\n",
            "Epoch: 1, Step : 240, LR : 1.072695035460993e-05, Avg Loss : 0.2502\n",
            "Epoch: 1, Step : 250, LR : 1.0549645390070923e-05, Avg Loss : 0.2835\n",
            "Epoch: 1, Step : 260, LR : 1.0372340425531916e-05, Avg Loss : 0.2193\n",
            "Epoch: 1, Step : 270, LR : 1.0195035460992908e-05, Avg Loss : 0.2524\n",
            "Epoch: 1, Step : 280, LR : 1.00177304964539e-05, Avg Loss : 0.2462\n",
            "Epoch 1 Total Mean Loss : 0.2550\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Epoch 1 Valid Loss : 0.3793 Valid Acc : 84.28\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "Saving epoch 1 checkpoint at /content/drive/MyDrive/Mymodules/model.ckpt.1\n",
            "*****Epoch 2 Train Start*****\n",
            "Epoch: 2, Step : 10, LR : 9.804964539007093e-06, Avg Loss : 0.2150\n",
            "Epoch: 2, Step : 20, LR : 9.627659574468086e-06, Avg Loss : 0.1835\n",
            "Epoch: 2, Step : 30, LR : 9.450354609929078e-06, Avg Loss : 0.1654\n",
            "Epoch: 2, Step : 40, LR : 9.273049645390073e-06, Avg Loss : 0.1569\n",
            "Epoch: 2, Step : 50, LR : 9.095744680851063e-06, Avg Loss : 0.1761\n",
            "Epoch: 2, Step : 60, LR : 8.918439716312058e-06, Avg Loss : 0.1856\n",
            "Epoch: 2, Step : 70, LR : 8.74113475177305e-06, Avg Loss : 0.1869\n",
            "Epoch: 2, Step : 80, LR : 8.563829787234044e-06, Avg Loss : 0.1360\n",
            "Epoch: 2, Step : 90, LR : 8.386524822695035e-06, Avg Loss : 0.1275\n",
            "Epoch: 2, Step : 100, LR : 8.20921985815603e-06, Avg Loss : 0.1571\n",
            "Epoch: 2, Step : 110, LR : 8.031914893617022e-06, Avg Loss : 0.1820\n",
            "Epoch: 2, Step : 120, LR : 7.854609929078016e-06, Avg Loss : 0.1231\n",
            "Epoch: 2, Step : 130, LR : 7.677304964539007e-06, Avg Loss : 0.2185\n",
            "Epoch: 2, Step : 140, LR : 7.500000000000001e-06, Avg Loss : 0.1736\n",
            "Epoch: 2, Step : 150, LR : 7.3226950354609935e-06, Avg Loss : 0.1600\n",
            "Epoch: 2, Step : 160, LR : 7.145390070921986e-06, Avg Loss : 0.1544\n",
            "Epoch: 2, Step : 170, LR : 6.968085106382979e-06, Avg Loss : 0.1846\n",
            "Epoch: 2, Step : 180, LR : 6.790780141843972e-06, Avg Loss : 0.1407\n",
            "Epoch: 2, Step : 190, LR : 6.613475177304965e-06, Avg Loss : 0.1440\n",
            "Epoch: 2, Step : 200, LR : 6.436170212765958e-06, Avg Loss : 0.1473\n",
            "Epoch: 2, Step : 210, LR : 6.258865248226951e-06, Avg Loss : 0.1873\n",
            "Epoch: 2, Step : 220, LR : 6.081560283687944e-06, Avg Loss : 0.1800\n",
            "Epoch: 2, Step : 230, LR : 5.904255319148937e-06, Avg Loss : 0.2161\n",
            "Epoch: 2, Step : 240, LR : 5.7269503546099295e-06, Avg Loss : 0.0936\n",
            "Epoch: 2, Step : 250, LR : 5.549645390070923e-06, Avg Loss : 0.1172\n",
            "Epoch: 2, Step : 260, LR : 5.372340425531915e-06, Avg Loss : 0.1202\n",
            "Epoch: 2, Step : 270, LR : 5.195035460992908e-06, Avg Loss : 0.2361\n",
            "Epoch: 2, Step : 280, LR : 5.017730496453901e-06, Avg Loss : 0.1537\n",
            "Epoch 2 Total Mean Loss : 0.1647\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "*****Epoch 2 Valid Start*****\n",
            "Epoch 2 Valid Loss : 0.4300 Valid Acc : 85.02\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "Saving epoch 2 checkpoint at /content/drive/MyDrive/Mymodules/model.ckpt.2\n",
            "*****Epoch 3 Train Start*****\n",
            "Epoch: 3, Step : 10, LR : 4.804964539007093e-06, Avg Loss : 0.0942\n",
            "Epoch: 3, Step : 20, LR : 4.6276595744680855e-06, Avg Loss : 0.0886\n",
            "Epoch: 3, Step : 30, LR : 4.450354609929078e-06, Avg Loss : 0.1203\n",
            "Epoch: 3, Step : 40, LR : 4.273049645390071e-06, Avg Loss : 0.1688\n",
            "Epoch: 3, Step : 50, LR : 4.095744680851064e-06, Avg Loss : 0.1134\n",
            "Epoch: 3, Step : 60, LR : 3.918439716312057e-06, Avg Loss : 0.0948\n",
            "Epoch: 3, Step : 70, LR : 3.74113475177305e-06, Avg Loss : 0.1133\n",
            "Epoch: 3, Step : 80, LR : 3.5638297872340426e-06, Avg Loss : 0.1567\n",
            "Epoch: 3, Step : 90, LR : 3.386524822695036e-06, Avg Loss : 0.0687\n",
            "Epoch: 3, Step : 100, LR : 3.2092198581560285e-06, Avg Loss : 0.0669\n",
            "Epoch: 3, Step : 110, LR : 3.031914893617022e-06, Avg Loss : 0.1278\n",
            "Epoch: 3, Step : 120, LR : 2.8546099290780144e-06, Avg Loss : 0.0887\n",
            "Epoch: 3, Step : 130, LR : 2.6773049645390077e-06, Avg Loss : 0.1013\n",
            "Epoch: 3, Step : 140, LR : 2.5e-06, Avg Loss : 0.1448\n",
            "Epoch: 3, Step : 150, LR : 2.322695035460993e-06, Avg Loss : 0.0858\n",
            "Epoch: 3, Step : 160, LR : 2.145390070921986e-06, Avg Loss : 0.1512\n",
            "Epoch: 3, Step : 170, LR : 1.968085106382979e-06, Avg Loss : 0.1189\n",
            "Epoch: 3, Step : 180, LR : 1.790780141843972e-06, Avg Loss : 0.1336\n",
            "Epoch: 3, Step : 190, LR : 1.6134751773049648e-06, Avg Loss : 0.1398\n",
            "Epoch: 3, Step : 200, LR : 1.4361702127659578e-06, Avg Loss : 0.1239\n",
            "Epoch: 3, Step : 210, LR : 1.2588652482269503e-06, Avg Loss : 0.0833\n",
            "Epoch: 3, Step : 220, LR : 1.0815602836879434e-06, Avg Loss : 0.0898\n",
            "Epoch: 3, Step : 230, LR : 9.042553191489363e-07, Avg Loss : 0.0933\n",
            "Epoch: 3, Step : 240, LR : 7.26950354609929e-07, Avg Loss : 0.1699\n",
            "Epoch: 3, Step : 250, LR : 5.496453900709221e-07, Avg Loss : 0.0912\n",
            "Epoch: 3, Step : 260, LR : 3.723404255319149e-07, Avg Loss : 0.0968\n",
            "Epoch: 3, Step : 270, LR : 1.9503546099290782e-07, Avg Loss : 0.1371\n",
            "Epoch: 3, Step : 280, LR : 1.773049645390071e-08, Avg Loss : 0.0762\n",
            "Epoch 3 Total Mean Loss : 0.1118\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "*****Epoch 3 Valid Start*****\n",
            "Epoch 3 Valid Loss : 0.4689 Valid Acc : 84.96\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "Saving epoch 3 checkpoint at /content/drive/MyDrive/Mymodules/model.ckpt.3\n",
            "Train Completed. End Program.\n"
          ]
        }
      ],
      "source": [
        "# 4 epoch 학습 (train_dataset 9000 / train_batch_size = 32 = 280 → 280 * 4)\n",
        "epochs=4\n",
        "model, optimizer, scheduler = initializer(train_dataloader, epochs)\n",
        "train(model, train_dataloader, valid_dataloader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Epoch 0 Valid Loss : 0.3664 Valid Acc : 84.34\n",
        "- Epoch 1 Valid Loss : 0.3793 Valid Acc : 84.28\n",
        "- Epoch 2 Valid Loss : 0.4300 Valid Acc : 85.02\n",
        "- Epoch 3 Valid Loss : 0.4689 Valid Acc : 84.96"
      ],
      "metadata": {
        "id": "AxxK67MhyJVq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T03:27:18.246441Z",
          "start_time": "2022-02-02T03:27:18.236617Z"
        },
        "id": "vA3_vqqCXccc"
      },
      "source": [
        "### 가장 dev **acc 성능**이 높았던 epoch의 모델의 체크 포인트를 불러와 로드하자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:22:27.646150Z",
          "start_time": "2022-02-02T06:22:26.945572Z"
        },
        "id": "mvfkSff25Q3z"
      },
      "outputs": [],
      "source": [
        "# torch.load 함수 사용 (epoch 2의 check_point)\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/Mymodules/model.ckpt.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:22:36.415665Z",
          "start_time": "2022-02-02T06:22:36.407250Z"
        },
        "id": "YqcxMmTj5Q3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08995329-551d-4d15-8e9f-21005c6ad8a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# checkpoint의 key 종류를 확인\n",
        "checkpoint.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:22:40.272939Z",
          "start_time": "2022-02-02T06:22:37.010491Z"
        },
        "id": "wTvFYgNi5Q30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54eefb17-c28c-4383-f7a6-85bab358b924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 1 epochs: 282\n"
          ]
        }
      ],
      "source": [
        "# 위에서 구현한 initializer 함수 사용하여 model, optimizer, scheduler 초기화\n",
        "\n",
        "epochs=1\n",
        "model, optimizer, scheduler = initializer(train_dataloader, epochs) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:22:40.443912Z",
          "start_time": "2022-02-02T06:22:40.274323Z"
        },
        "id": "CtR2sTW55Q30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9466cf6c-fa9c-4626-f12b-9ab652c3180d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "model.load_state_dict(checkpoint[\"model_state_dict\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzske7SR5Q30"
      },
      "source": [
        "### 모델 예측 함수 구현\n",
        "- test_dataloader를 입력받아 모델이 예측한 확률값 (probs)과 실제 정답 (label) 을 출력하는 `predict()` 함수를 구현하자.\n",
        "- 함수 정의\n",
        "  - 입력 매개변수\n",
        "    - `model` : `CustomClassifier` 모델. logits를 반환함 \n",
        "    - `test_dataloader` : test 데이터셋의 텍스트와 레이블을 배치로 갖는 dataloader\n",
        "  - 조건\n",
        "    - `test_dataloader`는 이터레이터기 때문에 이터레이터를 순회하면서 `all_logits` 리스트에 배치 단위의 logits를 저장하고 `all_labels` 리스트에 배치 단위의 레이블 (0 또는 1 값)을 저장하라\n",
        "  - 반환값\n",
        "    - `probs`\n",
        "      - logits에 softmax 함수를 취한 확률값. (test data 개수, label 개수) shape을 가짐. \bnp.array 타입으로 데이터 타입을 변환할 것.\n",
        "    - `labels`\n",
        "      - 0 또는 1 값을 갖는 np.array. (test data 개수,) shape을 가짐."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:22:48.062229Z",
          "start_time": "2022-02-02T06:22:48.057531Z"
        },
        "id": "yQ7WiD1Oigg9"
      },
      "outputs": [],
      "source": [
        "def predict(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    test_dataloader의 label별 확률값과 실제 label 값을 반환\n",
        "    \"\"\"\n",
        "\n",
        "    # model을 eval 모드로 설정 & device 할당\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = [] \n",
        "\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        # batch_input을 device 할당\n",
        "        batch_input.to(device)     \n",
        "\n",
        "        # gradient 계산하지 않고 forward 진행 (train 과정이 아니라, test 과정이기 때문.)\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # model에 batch_input을 넣어 logit 반환 & all_logits, all_labels 리스트에 값 추가 \n",
        "            logits = model(batch_input['input_ids'], batch_input['attention_mask'], batch_input['token_type_ids'])\n",
        "            logits = F.softmax(logits, dim=1)\n",
        "            all_logits.append(logits)\n",
        "\n",
        "            all_labels.append(batch_label)\n",
        "\n",
        "    probs = torch.cat(all_logits, dim=0).cpu().numpy() # logits을 확률값으로 변환 & Tensor 타입을 numpy.array 타입으로 변환 (2차원 np.array)\n",
        "    all_labels = torch.cat(all_labels, dim=0).cpu().numpy() #  Tensor 타입을 numpy.array 타입으로 변환 (1차원 np.array)\n",
        "\n",
        "    return probs, all_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델이 예측한 확률값과 실제 label을 입력 받아 정확도를 출력하는 **accuracy()** 함수를 구현하자. \n",
        "- 함수 정의 \n",
        "  - 입력 매개변수 \n",
        "    - `probs` : `predict()` 함수의 반환값. 2차원의 np.array\n",
        "    - `labels` : `predict()` 함수의 반환값. 1차원의 np.array\n",
        "  - 조건\n",
        "    - `probs`의 확률값이 0.5 이상이면 1, 이하이면 0이 되도록 만든다. 모델이 예측한 레이블을 실제값(`labels`)과 비교해 예측값과 실제값이 같으면 1, 다르면 0 점수를 준다. 모든 데이터에 대해 점수의 평균값이 accuracy 값이다. \n",
        "  - 반환값 \n",
        "    - `acc` : 정확도 (Float type)"
      ],
      "metadata": {
        "id": "lOxCjZ2g6ZeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy 함수 구현\n",
        "def accuracy(probs, labels):\n",
        "    y_pred = np.argmax(probs, axis=1).flatten() # probs(확률값)을 label로 변경(0.5 이상이면 1, 0.5 미만이면 0)\n",
        "    acc = np.sum(y_pred == labels) / len(labels) # 정확도 계산\n",
        "    return acc "
      ],
      "metadata": {
        "id": "3PnJZoY2wuEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:24:22.752497Z",
          "start_time": "2022-02-02T06:22:48.652784Z"
        },
        "id": "SwkrRPAhjsXb"
      },
      "outputs": [],
      "source": [
        "probs, labels = predict(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:24:22.759367Z",
          "start_time": "2022-02-02T06:24:22.753997Z"
        },
        "id": "MxDI8PRA5Q32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e339ec15-ef6a-4a8f-e9f2-2454fd1929b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.864"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "accuracy(probs, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqUfkx-5Q33"
      },
      "source": [
        "### `sklearn.metrics`의 `accuracy_score`, `roc_auc_score` 함수를 이용해 정확도와 auc를 계산하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:24:23.111879Z",
          "start_time": "2022-02-02T06:24:22.760568Z"
        },
        "id": "VFWj4lcp5Q33"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(probs, axis=1).flatten()"
      ],
      "metadata": {
        "id": "ppqas8G49P1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:24:23.116872Z",
          "start_time": "2022-02-02T06:24:23.113064Z"
        },
        "id": "p9BEe2mflTem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b35d84-953b-45a9-e89b-5666444b3d74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.864"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "# 정확도 출력\n",
        "accuracy_score(labels, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-02T06:24:23.125650Z",
          "start_time": "2022-02-02T06:24:23.117847Z"
        },
        "id": "oCl6BiPGpCPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e82888-19e7-434e-ec56-779ffb85d19e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8639999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "# auc 출력\n",
        "\n",
        "roc_auc_score(labels, y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "신현지_Week2_4_assginment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d321ab52be094027af6db242ff73413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_56c853b0280e441c90ffb43fa4954011",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0e63dd2edc114376a60458b8797771aa",
              "IPY_MODEL_b3de612915a24db89422dcae7f0f0b9b",
              "IPY_MODEL_95aa2a43b7454684b75e83185467ff47"
            ]
          }
        },
        "56c853b0280e441c90ffb43fa4954011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e63dd2edc114376a60458b8797771aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51099c4718414830bd21e515e6071658",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d414b3aa3ae442e8ec3a3f87e6eac38"
          }
        },
        "b3de612915a24db89422dcae7f0f0b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0acbbe906da54f21abadc65c8b834256",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 248477,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 248477,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2111629362c4f509db209af951162ac"
          }
        },
        "95aa2a43b7454684b75e83185467ff47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12d295e6f0f942c4885d178080cd5026",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 243k/243k [00:00&lt;00:00, 1.44MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8311bf8af344468e8ecd29f301f09dac"
          }
        },
        "51099c4718414830bd21e515e6071658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d414b3aa3ae442e8ec3a3f87e6eac38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0acbbe906da54f21abadc65c8b834256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2111629362c4f509db209af951162ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12d295e6f0f942c4885d178080cd5026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8311bf8af344468e8ecd29f301f09dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4c086c01008416ca577d8a5e25102ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3dd49f1ad704bdfa3b9ff4bd4ce8226",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd22a92c51df4466a302a4e5bcf15fd0",
              "IPY_MODEL_b4556b72d07a4ddeb49986890571908b",
              "IPY_MODEL_583194522ba94cdc8ff5c6faca34c3bb"
            ]
          }
        },
        "f3dd49f1ad704bdfa3b9ff4bd4ce8226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd22a92c51df4466a302a4e5bcf15fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dc3620bdaeb4847b87d72d165180145",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d4a53b7fba240778b6f2dfb071ac3d7"
          }
        },
        "b4556b72d07a4ddeb49986890571908b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74f3645f04904775ae3599228c9210dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 125,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 125,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_672c3b15df37497c9b4ed0f61faa204b"
          }
        },
        "583194522ba94cdc8ff5c6faca34c3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12ab5a5d11a44714bca975d5f4dbf985",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 125/125 [00:00&lt;00:00, 2.93kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88fbab4df4034b0d82b1ad4e978b7bcf"
          }
        },
        "6dc3620bdaeb4847b87d72d165180145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d4a53b7fba240778b6f2dfb071ac3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74f3645f04904775ae3599228c9210dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "672c3b15df37497c9b4ed0f61faa204b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12ab5a5d11a44714bca975d5f4dbf985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88fbab4df4034b0d82b1ad4e978b7bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20d3435db01a4b0fac9b477aeb87662d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d2de39a2f8b45bb8fca40c35df93e45",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e05a6d35e274afdbc189b02424a1b3b",
              "IPY_MODEL_b7126dea45df4230a2098b97de1bdd4c",
              "IPY_MODEL_a485175227d24c59b60407a55408d3cd"
            ]
          }
        },
        "8d2de39a2f8b45bb8fca40c35df93e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e05a6d35e274afdbc189b02424a1b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9511c53f58944349879b8767e24b6df9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef76cb2fe0404ec5b11b698ea2fd0549"
          }
        },
        "b7126dea45df4230a2098b97de1bdd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6018b7bd6d34a2d9c27b1e158f453f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e0696922bb347a18cca1eafb8478cf0"
          }
        },
        "a485175227d24c59b60407a55408d3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_201c39ee3f3b4f15a244c6c20e7d31cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 289/289 [00:00&lt;00:00, 7.02kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76f4595fcee24b81ac6f6a721b17b4d4"
          }
        },
        "9511c53f58944349879b8767e24b6df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef76cb2fe0404ec5b11b698ea2fd0549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6018b7bd6d34a2d9c27b1e158f453f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e0696922bb347a18cca1eafb8478cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "201c39ee3f3b4f15a244c6c20e7d31cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76f4595fcee24b81ac6f6a721b17b4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8635b140a5bb409eb2b4d3ca140998e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f5a4fae5dee49a5af69ce6ff665f4a1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8095139339064856a0a90a0b593a154c",
              "IPY_MODEL_1978e4da33994b36901469b5557a42e7",
              "IPY_MODEL_d1623787f4f74648bd84828f433ef52d"
            ]
          }
        },
        "6f5a4fae5dee49a5af69ce6ff665f4a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8095139339064856a0a90a0b593a154c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3828164fe1974f2eb130fe631a3e93a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6e79b8996104020882a683107d7ba03"
          }
        },
        "1978e4da33994b36901469b5557a42e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2318b75e89f949bca31fd89fbee43be0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 425,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 425,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0aed0d693f2b48a8b96d82e1d072c129"
          }
        },
        "d1623787f4f74648bd84828f433ef52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d3d1c356ed14751b6ad125448b4e100",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 425/425 [00:00&lt;00:00, 10.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13e1d0c68d904ad4bf26ad1c1e0b55d1"
          }
        },
        "3828164fe1974f2eb130fe631a3e93a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6e79b8996104020882a683107d7ba03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2318b75e89f949bca31fd89fbee43be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0aed0d693f2b48a8b96d82e1d072c129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d3d1c356ed14751b6ad125448b4e100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13e1d0c68d904ad4bf26ad1c1e0b55d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f1f9e5305a0409bad443be219fa9d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75e709e7009c4a81b0b8875abb75793b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c1be09835754bbc9869b566d440d64e",
              "IPY_MODEL_3807d440dde3429ca7d9ce88ca60c0ed",
              "IPY_MODEL_80d432ca46fc40dbb1efdbafe5fc057e"
            ]
          }
        },
        "75e709e7009c4a81b0b8875abb75793b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c1be09835754bbc9869b566d440d64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a4448fe121d4aaca1f0864f3d54fe77",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20b0b597d83640c9bef7b865f8b2cbc4"
          }
        },
        "3807d440dde3429ca7d9ce88ca60c0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9d70f90f5ca4631bdd6660180d10314",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 445025130,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 445025130,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4faf29530d1e4c08aed80615a12b7381"
          }
        },
        "80d432ca46fc40dbb1efdbafe5fc057e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f83110d2ed8245278b94d1426602fd6f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 424M/424M [00:13&lt;00:00, 29.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4827d1f93fb14b55967034c79ce3c719"
          }
        },
        "1a4448fe121d4aaca1f0864f3d54fe77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20b0b597d83640c9bef7b865f8b2cbc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9d70f90f5ca4631bdd6660180d10314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4faf29530d1e4c08aed80615a12b7381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f83110d2ed8245278b94d1426602fd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4827d1f93fb14b55967034c79ce3c719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}